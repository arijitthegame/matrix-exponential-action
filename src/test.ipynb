{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conjugate_gradient as CG\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "import scipy.sparse.linalg as splinalg\n",
    "import numpy as np\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chebyshev(x, degree):\n",
    "    retvar = torch.zeros(x.size(0), degree+1).type(x.type())\n",
    "    retvar[:, 0] = x * 0 + 1\n",
    "    if degree > 0:\n",
    "        retvar[:, 1] = x\n",
    "        for ii in range(1, degree):\n",
    "            retvar[:, ii+1] = 2 * x * retvar[:, ii] -  retvar[:, ii-1]\n",
    "\n",
    "    return retvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10)*2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6714, -0.4194,  0.2677, -0.0922, -0.5779,  0.4677,  0.9019,  0.2290,\n",
       "         0.3846, -0.8832], requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = chebyshev(x, degree=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.6714, -0.0985, -0.8036, -0.9806],\n",
       "        [ 1.0000, -0.4194, -0.6483,  0.9631, -0.1595],\n",
       "        [ 1.0000,  0.2677, -0.8566, -0.7264,  0.4677],\n",
       "        [ 1.0000, -0.0922, -0.9830,  0.2733,  0.9326],\n",
       "        [ 1.0000, -0.5779, -0.3321,  0.9617, -0.7794],\n",
       "        [ 1.0000,  0.4677, -0.5626, -0.9939, -0.3670],\n",
       "        [ 1.0000,  0.9019,  0.6268,  0.2286, -0.2143],\n",
       "        [ 1.0000,  0.2290, -0.8951, -0.6389,  0.6025],\n",
       "        [ 1.0000,  0.3846, -0.7042, -0.9262, -0.0081],\n",
       "        [ 1.0000, -0.8832,  0.5600, -0.1060, -0.3727]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:, 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y[:, 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chebyshev(torch.nn.Module):\n",
    "    def __init__(self, M=2):\n",
    "        super().__init__()\n",
    "        self.register_buffer('M', torch.tensor([float(M)]))\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = Non_zero().apply(inp)\n",
    "        return chebyshev().apply(inp, self.M)\n",
    "\n",
    "    def init_ident(self):\n",
    "        with torch.no_grad():\n",
    "            self.M = torch.ones_like(self.M)\n",
    "        return self\n",
    "\n",
    "# prevents nan\n",
    "class Non_zero(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, inp):\n",
    "        # if  0 add 1e-7 to it\n",
    "        offset = (inp == 0).float()*(1e-7)\n",
    "        return inp + offset\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, outp):\n",
    "        return outp\n",
    "\n",
    "\n",
    "class chebyshev(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, inp, M):\n",
    "        \n",
    "        indices = torch.tensor(range(inp.size()[-1])).reshape(-1,2).t()\n",
    "        \n",
    "        outp = torch.empty_like(inp)\n",
    "        \n",
    "        #reused indexing/computations\n",
    "        xi = inp[..., indices[0]]  \n",
    "        xj = inp[..., indices[1]]\n",
    "        \n",
    "        \n",
    "        x_norm = torch.sqrt(xi**2 + xj**2) \n",
    "\n",
    "        # trig form, clamp input to acos to prevent edge case with floats\n",
    "        M_angle = M * torch.acos((xi / x_norm).clamp(min=-1.,max=1.))\n",
    "        chebyt_outp = torch.cos(M_angle)\n",
    "        chebyu_outp = torch.sin(M_angle)\n",
    "        \n",
    "        # function implementation\n",
    "        outp[...,indices[0]] = x_norm / torch.sqrt(M) * chebyt_outp\n",
    "        outp[...,indices[1]] = xj.sign() * x_norm / torch.sqrt(M) * chebyu_outp\n",
    "        \n",
    "        ctx.save_for_backward(xi, xj, x_norm ** 2, M, indices, outp)\n",
    "        return outp\n",
    "     \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_L_y):\n",
    "        xi, xj, x2_norm, M, indices, outp = ctx.saved_tensors\n",
    "        #read grad_a_b as the derivitive of a w.r.t b\n",
    "\n",
    "        # split function output\n",
    "        yi = outp[..., indices[0]]\n",
    "        yj = outp[..., indices[1]]\n",
    "        \n",
    "        # function gradient computation w.r.t. inputs\n",
    "        grad_yi_xi = (xi * yi + M * xj * yj) / x2_norm\n",
    "        grad_yj_xi = (-M * xj * yi + xi * yj) / x2_norm\n",
    "        grad_yi_xj = (xj * yi + -M * xi * yj) / x2_norm\n",
    "        grad_yj_xj = (M * xi * yi + xj * yj) / x2_norm\n",
    "        \n",
    "        # given gradients\n",
    "        grad_L_yi = grad_L_y[..., indices[0]]\n",
    "        grad_L_yj = grad_L_y[..., indices[1]]\n",
    "\n",
    "        # chain rule\n",
    "        grad_L_xi = grad_L_yi * grad_yi_xi + grad_L_yj * grad_yj_xi\n",
    "        grad_L_xj = grad_L_yi * grad_yi_xj + grad_L_yj * grad_yj_xj\n",
    "        \n",
    "        # splice gradients together\n",
    "        grad_L_x = torch.empty_like(grad_L_y) \n",
    "        \n",
    "        grad_L_x[..., indices[0]] = grad_L_xi\n",
    "        grad_L_x[..., indices[1]] = grad_L_xj\n",
    "        return grad_L_x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = Chebyshev(M=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2435, -0.3121,  0.0343, -0.1374, -0.3394, -0.1516,  0.2535,  0.3901,\n",
       "        -0.0346,  0.4804], grad_fn=<chebyshevBackward>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2435, -0.3121,  0.0343, -0.1374, -0.3394, -0.1516,  0.2535,  0.3901,\n",
       "        -0.0346,  0.4804], grad_fn=<chebyshevBackward>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chebyshev().apply(x, torch.tensor([float(4)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.diag(torch.rand(10))\n",
    "B = torch.rand(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## batchify the above\n",
    "X = rearrange(X, 'm n -> 1 m n')\n",
    "B = rearrange(B, 'm n -> 1 m n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9773],\n",
       "         [0.3413],\n",
       "         [0.3760],\n",
       "         [0.4546],\n",
       "         [0.5687],\n",
       "         [0.5831],\n",
       "         [0.6299],\n",
       "         [0.3112],\n",
       "         [0.4156],\n",
       "         [0.5679]]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.requires_grad_(False)\n",
    "B.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = CG.CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = cg.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 1])\n",
      "torch.Size([1, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "s = f(X,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = s.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.8478],\n",
       "         [1.4370],\n",
       "         [0.5429],\n",
       "         [0.6381],\n",
       "         [8.4948],\n",
       "         [0.6338],\n",
       "         [0.7111],\n",
       "         [0.4144],\n",
       "         [1.4915],\n",
       "         [1.2664]]], grad_fn=<CGBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.9773],\n",
      "         [0.3413],\n",
      "         [0.3760],\n",
      "         [0.4546],\n",
      "         [0.5687],\n",
      "         [0.5831],\n",
      "         [0.6299],\n",
      "         [0.3112],\n",
      "         [0.4156],\n",
      "         [0.5679]]], requires_grad=True), tensor([[[2.8478],\n",
      "         [1.4370],\n",
      "         [0.5429],\n",
      "         [0.6381],\n",
      "         [8.4948],\n",
      "         [0.6338],\n",
      "         [0.7111],\n",
      "         [0.4144],\n",
      "         [1.4915],\n",
      "         [1.2664]]], grad_fn=<CGBackward>))\n",
      "tensor([[[ 2.9140],\n",
      "         [ 4.2104],\n",
      "         [ 1.4439],\n",
      "         [ 1.4036],\n",
      "         [14.9363],\n",
      "         [ 1.0868],\n",
      "         [ 1.1289],\n",
      "         [ 1.3314],\n",
      "         [ 3.5889],\n",
      "         [ 2.2298]]])\n"
     ]
    }
   ],
   "source": [
    "s1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<attribute 'saved_variables' of 'torch._C._FunctionBase' objects>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CG.CG.saved_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_numpy_to_torch(A):\n",
    "    rows, cols = A.nonzero()\n",
    "    values = A.data\n",
    "    indices = np.vstack((rows, cols))\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.DoubleTensor(values)\n",
    "    return torch.sparse.DoubleTensor(i, v, A.shape)\n",
    "\n",
    "n = 50\n",
    "m = 50\n",
    "K = 1\n",
    "As = [nx.laplacian_matrix(\n",
    "    nx.gnm_random_graph(n, 20 * n)) + .1 * sparse.eye(n) for _ in range(K)]\n",
    "Ms = [sparse.diags(1. / A.diagonal(), format='csc') for A in As]\n",
    "A_bdiag = sparse.block_diag(As)\n",
    "M_bdiag = sparse.block_diag(Ms)\n",
    "Bs = [np.random.randn(n, m) for _ in range(K)]\n",
    "As_torch = [None] * K\n",
    "Ms_torch = [None] * K\n",
    "B_torch = torch.DoubleTensor(K, n, m).requires_grad_()\n",
    "A_bdiag_torch = sparse_numpy_to_torch(A_bdiag)\n",
    "M_bdiag_torch = sparse_numpy_to_torch(M_bdiag)\n",
    "\n",
    "for i in range(K):\n",
    "    As_torch[i] = sparse_numpy_to_torch(As[i])\n",
    "    Ms_torch[i] = sparse_numpy_to_torch(Ms[i])\n",
    "    B_torch[i] = torch.tensor(Bs[i])\n",
    "\n",
    "\n",
    "def A_bmm(X):\n",
    "    Y = [(As_torch[i]@X[i]).unsqueeze(0) for i in range(K)]\n",
    "    return torch.cat(Y, dim=0)\n",
    "\n",
    "\n",
    "def M_bmm(X):\n",
    "    Y = [(Ms_torch[i]@X[i]).unsqueeze(0) for i in range(K)]\n",
    "    return torch.cat(Y, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[ 0,  0,  0,  ..., 49, 49, 49],\n",
       "                       [ 0,  1,  2,  ..., 46, 47, 49]]),\n",
       "       values=tensor([41.1000, -1.0000, -1.0000,  ..., -1.0000, -1.0000,\n",
       "                      36.1000]),\n",
       "       size=(50, 50), nnz=2050, dtype=torch.float64, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "As_torch[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.4912],\n",
       "          [0.9723],\n",
       "          [0.6954],\n",
       "          [0.8210],\n",
       "          [1.7861],\n",
       "          [0.8587],\n",
       "          [2.3634],\n",
       "          [3.5718],\n",
       "          [1.5046],\n",
       "          [1.9391]]]), {'niter': 7, 'optimal': True})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CG.cg_batch(X, B, X0=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat(x0, ' h c -> 1 h c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-6\n",
    "def _lanczos_layer(A, num_eig_vec, mask=None, use_reorthogonalization=False):\n",
    "    \"\"\" Lanczos for symmetric matrix A\n",
    "    \n",
    "      Args:\n",
    "        A: float tensor, shape B X N X N\n",
    "        mask: float tensor, shape B X N\n",
    "        num_eig_vec = K\n",
    "      Returns:\n",
    "      T: shape B X K X K, tridiagonal matrix\n",
    "      Q: shape B X N X K, orthonormal matrix\n",
    "      \n",
    "    \"\"\"\n",
    "    batch_size = A.shape[0]\n",
    "    num_node = A.shape[1]\n",
    "    lanczos_iter = min(num_node, num_eig_vec)\n",
    "\n",
    "    # initialization\n",
    "    alpha = [None] * (lanczos_iter + 1)\n",
    "    beta = [None] * (lanczos_iter + 1)\n",
    "    Q = [None] * (lanczos_iter + 2)\n",
    "\n",
    "    beta[0] = torch.zeros(batch_size, 1, 1).to(A.device)\n",
    "    Q[0] = torch.zeros(batch_size, num_node, 1).to(A.device)\n",
    "    Q[1] = torch.randn(batch_size, num_node, 1).to(A.device)\n",
    "\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(dim=2).float()\n",
    "        Q[1] = Q[1] * mask\n",
    "\n",
    "    Q[1] = Q[1] / torch.norm(Q[1], 2, dim=1, keepdim=True)\n",
    "\n",
    "    # Lanczos loop\n",
    "    lb = 1.0e-4\n",
    "    valid_mask = []\n",
    "    for ii in range(1, lanczos_iter + 1):\n",
    "      z = torch.bmm(A, Q[ii])  # shape B X N X 1\n",
    "      alpha[ii] = torch.sum(Q[ii] * z, dim=1, keepdim=True)  # shape B X 1 X 1\n",
    "      z = z - alpha[ii] * Q[ii] - beta[ii - 1] * Q[ii - 1]  # shape B X N X 1\n",
    "\n",
    "      if use_reorthogonalization and ii > 1:\n",
    "        # N.B.: Gram Schmidt does not bring significant difference of performance\n",
    "        def _gram_schmidt(xx, tt):\n",
    "          # xx shape B X N X 1\n",
    "          for jj in range(1, tt):\n",
    "            xx = xx - torch.sum(\n",
    "                xx * Q[jj], dim=1, keepdim=True) / (\n",
    "                    torch.sum(Q[jj] * Q[jj], dim=1, keepdim=True) + EPS) * Q[jj]\n",
    "          return xx\n",
    "\n",
    "        # do Gram Schmidt process twice\n",
    "        for _ in range(2):\n",
    "          z = _gram_schmidt(z, ii)\n",
    "\n",
    "      beta[ii] = torch.norm(z, p=2, dim=1, keepdim=True)  # shape B X 1 X 1\n",
    "\n",
    "      # N.B.: once lanczos fails at ii-th iteration, all following iterations\n",
    "      # are doomed to fail\n",
    "      tmp_valid_mask = (beta[ii] >= lb).float()  # shape\n",
    "      if ii == 1:\n",
    "        valid_mask += [tmp_valid_mask]\n",
    "      else:\n",
    "        valid_mask += [valid_mask[-1] * tmp_valid_mask]\n",
    "\n",
    "      # early stop\n",
    "      Q[ii + 1] = (z * valid_mask[-1]) / (beta[ii] + EPS)\n",
    "\n",
    "    # get alpha & beta\n",
    "    alpha = torch.cat(alpha[1:], dim=1).squeeze(dim=2)  # shape B X T\n",
    "    beta = torch.cat(beta[1:-1], dim=1).squeeze(dim=2)  # shape B X (T-1)\n",
    "\n",
    "    valid_mask = torch.cat(valid_mask, dim=1).squeeze(dim=2)  # shape B X T\n",
    "    idx_mask = torch.sum(valid_mask, dim=1).long()\n",
    "    if mask is not None:\n",
    "      idx_mask = torch.min(idx_mask, torch.sum(mask, dim=1).squeeze().long())\n",
    "\n",
    "    for ii in range(batch_size):\n",
    "      if idx_mask[ii] < valid_mask.shape[1]:\n",
    "        valid_mask[ii, idx_mask[ii]:] = 0.0\n",
    "\n",
    "    # remove spurious columns\n",
    "    alpha = alpha * valid_mask\n",
    "    beta = beta * valid_mask[:, :-1]\n",
    "\n",
    "    T = []\n",
    "    for ii in range(batch_size):\n",
    "      T += [\n",
    "          torch.diag(alpha[ii]) + torch.diag(beta[ii], diagonal=1) + torch.diag(\n",
    "              beta[ii], diagonal=-1)\n",
    "      ]\n",
    "\n",
    "    T = torch.stack(T, dim=0)  # shape B X T X T\n",
    "    Q = torch.cat(Q[1:-1], dim=2)  # shape B X N X T\n",
    "    Q_mask = valid_mask.unsqueeze(dim=1).repeat(1, Q.shape[1], 1)\n",
    "\n",
    "    # remove spurious rows\n",
    "    for ii in range(batch_size):\n",
    "      if idx_mask[ii] < Q_mask.shape[1]:\n",
    "        Q_mask[ii, idx_mask[ii]:, :] = 0.0\n",
    "\n",
    "    Q = Q * Q_mask\n",
    "\n",
    "    # pad 0 when necessary\n",
    "    if lanczos_iter < num_eig_vec:\n",
    "      pad = (0, num_eig_vec - lanczos_iter, 0,\n",
    "             num_eig_vec - lanczos_iter)\n",
    "      T = F.pad(T, pad)\n",
    "      pad = (0, self.num_eig_vec - lanczos_iter)\n",
    "      Q = F.pad(Q, pad)\n",
    "\n",
    "    return T, Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(30,30)\n",
    "A1 = A + A.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, Q = _lanczos_layer(A1.reshape(1,30,30), 5, use_reorthogonalization=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import solve\n",
    "\n",
    "def jacobi(A, b, x, n):\n",
    "\n",
    "    D = np.diag(A)\n",
    "    R = A - np.diagflat(D)\n",
    "    \n",
    "    for i in range(n):\n",
    "        x = (b - np.dot(R,x))/ D\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = A + A.transpose() + 20*np.identity(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.rand(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.63298592e-02,  9.01206811e-04,  2.74598641e-02,  3.90240159e-03,\n",
       "        9.35773454e-05, -9.70254066e-04,  3.48676971e-02,  2.38328224e-02,\n",
       "        1.04884637e-02, -3.68662329e-03,  2.64929282e-02,  4.32761896e-03,\n",
       "        2.63773283e-02,  3.00777617e-02,  1.74043244e-03,  2.86262706e-02,\n",
       "        3.57581306e-02, -7.97009305e-03, -1.06529760e-03,  1.58886845e-02])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobi(A1,b,x, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.63298592e-02,  9.01206810e-04,  2.74598641e-02,  3.90240159e-03,\n",
       "        9.35773445e-05, -9.70254067e-04,  3.48676970e-02,  2.38328224e-02,\n",
       "        1.04884637e-02, -3.68662329e-03,  2.64929282e-02,  4.32761895e-03,\n",
       "        2.63773283e-02,  3.00777617e-02,  1.74043244e-03,  2.86262706e-02,\n",
       "        3.57581306e-02, -7.97009305e-03, -1.06529760e-03,  1.58886845e-02])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve(A1,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
